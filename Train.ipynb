{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e05389-f87b-4597-9883-2ef1c6e78392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Select relevant columns for training\n",
    "selected_columns = [\n",
    "    \"Active Mean\", \"Total Fwd Packets\", \"tot_dur\", \"flows\", \"pktrate\", \"tx_bytes\", \"rx_bytes\", \"tx_kbps\", \"rx_kbps\", \"tot_kbps\"\n",
    "]\n",
    "X = df[selected_columns]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data for LSTM (samples, timesteps, features)\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=False, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save(\"lstm_model.h5\")\n",
    "\n",
    "# Load the model for predictions\n",
    "loaded_model = tf.keras.models.load_model(\"lstm_model.h5\")\n",
    "\n",
    "# Make a prediction for a new sample\n",
    "def make_prediction(input_data):\n",
    "    input_scaled = scaler.transform([input_data])\n",
    "    input_reshaped = input_scaled.reshape((1, 1, len(input_data)))\n",
    "    prediction = loaded_model.predict(input_reshaped)\n",
    "    return {column: value for column, value in zip(selected_columns, input_data)}, prediction[0][0]\n",
    "\n",
    "# Example usage\n",
    "new_sample = [100, 716000000, 1.01e11, 3, 451, 143928631, 3917, 0, 0.0, 0.0]\n",
    "columns_with_values, result = make_prediction(new_sample)\n",
    "print(\"Input Columns and Values:\")\n",
    "for column, value in columns_with_values.items():\n",
    "    print(f\"{column}: {value}\")\n",
    "print(\"Predicted Label:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77f876-2a43-4210-a75b-bf482f86f4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dc085-31a6-418c-a14a-03aa783168b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Select relevant columns for training\n",
    "selected_columns = [\n",
    "    \"Active Mean\", \"Total Fwd Packets\", \"tot_dur\", \"flows\", \"pktrate\", \"tx_bytes\", \"rx_bytes\", \"tx_kbps\", \"rx_kbps\", \"tot_kbps\"\n",
    "]\n",
    "X = df[selected_columns]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data for LSTM (samples, timesteps, features)\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model with improvements\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=False, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model with validation data and the learning rate scheduler callback\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[lr_scheduler])\n",
    "\n",
    "# Save the model\n",
    "model.save(\"improved_lstm_model.h5\")\n",
    "\n",
    "# Load the model for predictions\n",
    "loaded_model = tf.keras.models.load_model(\"improved_lstm_model.h5\")\n",
    "\n",
    "# Make a prediction for a new sample\n",
    "def make_prediction(input_data):\n",
    "    input_scaled = scaler.transform([input_data])\n",
    "    input_reshaped = input_scaled.reshape((1, 1, len(input_data)))\n",
    "    prediction = loaded_model.predict(input_reshaped)\n",
    "    return {column: value for column, value in zip(selected_columns, input_data)}, prediction[0][0]\n",
    "\n",
    "# Example usage\n",
    "new_sample = [100, 716000000, 1.01e11, 3, 451, 143928631, 3917, 0, 0.0, 0.0]\n",
    "columns_with_values, result = make_prediction(new_sample)\n",
    "\n",
    "# Output the results\n",
    "print(\"Input Columns and Values:\")\n",
    "for column, value in columns_with_values.items():\n",
    "    print(f\"{column}: {value}\")\n",
    "print(\"Predicted Label:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae850c2-2baf-40fd-b664-3ea701dfcbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7ed2f-6727-4a27-9448-e14428eb4883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99557dd3-934c-44fe-a0d6-423d37782bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset:\n",
      "['Source IP', 'Source Port', 'src', 'dst', 'Destination IP ', 'Destination Port ', 'Active Mean', 'Total Fwd Packets', 'tot_dur', 'flows', 'FIN Flag Count', 'pktperflow', 'byteperflow', 'pktrate', 'Pairflow', 'Protocol', 'port_no', 'tx_bytes', 'rx_bytes', 'tx_kbps', 'rx_kbps', 'tot_kbps', 'label']\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12712\n",
      "           1       1.00      1.00      1.00      8157\n",
      "\n",
      "    accuracy                           1.00     20869\n",
      "   macro avg       1.00      1.00      1.00     20869\n",
      "weighted avg       1.00      1.00      1.00     20869\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12705     7]\n",
      " [   10  8147]]\n",
      "Feature Importances:\n",
      "byteperflow          0.288964\n",
      "pktperflow           0.171412\n",
      "pktrate              0.153174\n",
      "FIN Flag Count       0.119160\n",
      "tot_dur              0.085780\n",
      "Active Mean          0.073299\n",
      "Protocol_TCP         0.024460\n",
      "Protocol_UDP         0.022774\n",
      "Total Fwd Packets    0.017212\n",
      "tx_bytes             0.015836\n",
      "flows                0.011158\n",
      "Pairflow             0.008654\n",
      "rx_bytes             0.008117\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Verify column names (to ensure no trailing spaces or typos)\n",
    "print(\"Column names in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Drop redundant/irrelevant columns (with corrected trailing spaces)\n",
    "columns_to_drop = ['Source IP', 'Source Port', 'src', 'dst', 'Destination IP ', \n",
    "                   'Destination Port ', 'tx_kbps', 'rx_kbps', 'tot_kbps', 'port_no']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Encode categorical 'Protocol' column\n",
    "encoder = OneHotEncoder(drop='first')  # Remove 'sparse=False'\n",
    "protocol_encoded = encoder.fit_transform(df[['Protocol']])\n",
    "df_protocol = pd.DataFrame(protocol_encoded.toarray(), columns=encoder.get_feature_names_out(['Protocol']))\n",
    "df = pd.concat([df.drop('Protocol', axis=1), df_protocol], axis=1)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Split into train/test sets (stratified due to class imbalance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest (handles class imbalance)\n",
    "model = RandomForestClassifier(\n",
    "    class_weight='balanced',  # Adjust for imbalanced classes\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance (optional)\n",
    "print(\"Feature Importances:\")\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(feature_importances.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0779a1-2673-4dd1-83e1-c7c3f7dae452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
